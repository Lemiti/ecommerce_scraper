# ğŸš€ E-commerce Product Scraper

A professional, configurable Python-based web scraper for extracting product data from e-commerce websites. Built for freelancers and businesses needing reliable data extraction.

## âœ¨ **Features**

### **Core Functionality**
- âœ… **Multi-page catalog scraping** with automatic pagination detection
- âœ… **Product data extraction**: name, price, description, images, variants, etc.
- âœ… **Error resilience**: Continues on individual product failures
- âœ… **Rate limiting**: Respectful scraping with configurable delays
- âœ… **Resume capability**: Save/load progress for large catalogs

### **Export Options**
- ğŸ“Š **Excel Export**: Clean, formatted `.xlsx` files with timestamps
- â˜ï¸ **Google Sheets Export**: Live cloud-based spreadsheets (API setup required)
- ğŸ–¼ï¸ **Image Downloader**: Automatic image downloading with compression
- ğŸ“ **Multiple Formats**: CSV, JSON coming soon

### **User Interfaces**
- ğŸ–¥ï¸ **Simple GUI**: Tkinter-based interface for non-technical users
- ğŸ’» **CLI Interface**: Command-line for automation and scripting
- âš™ï¸ **Configuration System**: JSON-based site configurations

### **Professional Features**
- ğŸ”’ **Compliance-aware**: Respects robots.txt and rate limits
- ğŸ“ **Comprehensive logging**: Detailed logs for debugging
- ğŸ§ª **Unit tests**: Core functionality tested
- ğŸ—ï¸ **Modular architecture**: Easy to extend and maintain

## ğŸ› ï¸ **Installation**

### **1. Clone & Setup**
```bash
git clone https://github.com/yourusername/ecommerce-scraper.git
cd ecommerce-scraper
```
#### Create virtual environment (recommended)
```bash
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate
```
#### Install dependencies
```bash
pip install -r requirements.txt
```
### **2. Quick Start**

```bash
python main.py

# Run in CLI mode
python main.py --cli

# Run GUI directly
python run_gui.py
```
## ğŸ“ **Project Structure**
```text
ecommerce_scraper/
â”œâ”€â”€ src/                          # Source code
â”‚   â”œâ”€â”€ core/                     # Core scraping engine
â”‚   â”‚   â”œâ”€â”€ scraper_engine.py     # Main scraping logic
â”‚   â”‚   â””â”€â”€ data_models.py        # Product data models
â”‚   â”œâ”€â”€ parsers/                  # HTML parsing
â”‚   â”‚   â””â”€â”€ bs4_parser.py         # BeautifulSoup parser
â”‚   â”œâ”€â”€ exporters/                # Data export modules
â”‚   â”‚   â”œâ”€â”€ excel_exporter.py     # Excel export
â”‚   â”‚   â”œâ”€â”€ google_sheets_exporter.py  # Google Sheets export
â”‚   â”‚   â””â”€â”€ image_downloader.py   # Image downloading
â”‚   â”œâ”€â”€ utils/                    # Utilities
â”‚   â”‚   â”œâ”€â”€ request_manager.py    HTTP requests with retry logic
â”‚   â”‚   â”œâ”€â”€ config_loader.py      # Configuration management
â”‚   â”‚   â””â”€â”€ logger.py             # Logging setup
â”‚   â””â”€â”€ interface/                # User interfaces
â”‚       â”œâ”€â”€ gui_interface.py      # Tkinter GUI
â”‚       â””â”€â”€ cli_interface.py      # Command-line interface
â”œâ”€â”€ configs/                      # Configuration files
â”‚   â”œâ”€â”€ default.json              # Default settings
â”‚   â””â”€â”€ website_templates/        # Site-specific configurations
â”œâ”€â”€ exports/                      # Output directory (auto-created)
â”‚   â”œâ”€â”€ products_*.xlsx           # Excel exports
â”‚   â””â”€â”€ images/                   # Downloaded images
â”œâ”€â”€ logs/                         # Log files (auto-created)
â”œâ”€â”€ tests/                        # Unit tests
â”œâ”€â”€ main.py                       # Main entry point
â”œâ”€â”€ run_gui.py                    # GUI entry point
â””â”€â”€ requirements.txt              # Python dependencies
```
